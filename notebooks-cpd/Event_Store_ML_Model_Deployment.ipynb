{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Db2 Event Store - Machine Learning Modeling and Model Deployment \n",
    "IBM Db2 Event Store is a hybrid transactional/analytical processing (HTAP) system. This notebook illustrates the machine learning modeling and model deployment using IBM Db2 Event Store.\n",
    "\n",
    "***Pre-Req: Event Store Data Analytics***\n",
    "\n",
    "When finish this demo, you will learn:\n",
    "- How to build a machine learning model\n",
    "- How to save and deploy the model\n",
    "- How to make realtime predictions with the deployed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to IBM Db2 Event Store\n",
    "\n",
    "\n",
    "Edit the values in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONNECTION_ENDPOINT=\"\"\n",
    "\n",
    "EVENT_USER_ID=\"\"\n",
    "\n",
    "EVENT_PASSWORD=\"\"\n",
    "\n",
    "# Port will be 1100 for version 1.1.2 or later (5555 for version 1.1.1)\n",
    "PORT = \"30370\"\n",
    "\n",
    "DEPLOYMENT_ID=\"\"\n",
    "\n",
    "# Database name\n",
    "DB_NAME = \"EVENTDB\"\n",
    "\n",
    "# Table name\n",
    "TABLE_NAME = \"IOT_TEMPERATURE\"\n",
    "\n",
    "HOSTNAME=\"\"\n",
    "\n",
    "DEPLOYMENT_SPACE=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bearerToken=!echo `curl --silent -k -X GET https://{HOSTNAME}:443/v1/preauth/validateAuth -u {EVENT_USER_ID}:{EVENT_PASSWORD} |python -c \"import sys, json; print(json.load(sys.stdin)['accessToken'])\"`\n",
    "bearerToken=bearerToken[0]\n",
    "keystorePassword=!echo `curl -k --silent  GET -H \"authorization: Bearer {bearerToken}\" \"https://{HOSTNAME}:443/icp4data-databases/{DEPLOYMENT_ID}/zen/com/ibm/event/api/v1/oltp/keystore_password\"`\n",
    "keystorePassword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: Only run this cell if your IBM Db2 Event Store is installed with IBM Cloud Pak for Data (CP4D)\n",
    "\n",
    "# In IBM Cloud Pak for Data, we need to create link to ensure Event Store Python library is \n",
    "# properly exposed to the Spark runtime.\n",
    "from pathlib import Path\n",
    "src = '/home/spark/user_home/eventstore/eventstore'\n",
    "dst = '/home/spark/shared/user-libs/python3.6/eventstore'\n",
    "is_symlink = Path(dst).is_symlink()\n",
    "if is_symlink == False :\n",
    "    os.symlink(src, dst)\n",
    "    print(\"Creating symlink to include Event Store Python library...\")\n",
    "else:\n",
    "    print(\"Symlink already exists, not creating..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eventstore.common import ConfigurationReader\n",
    "from eventstore.oltp import EventContext\n",
    "from eventstore.sql import EventSession\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Event Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfigurationReader.setConnectionEndpoints(CONNECTION_ENDPOINT)\n",
    "ConfigurationReader.setEventUser(EVENT_USER_ID)\n",
    "ConfigurationReader.setEventPassword(EVENT_PASSWORD)\n",
    "ConfigurationReader.setSslKeyAndTrustStorePasswords(keystorePassword[0])\n",
    "ConfigurationReader.setDeploymentID(DEPLOYMENT_ID)\n",
    "ConfigurationReader.getSslTrustStorePassword()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the database\n",
    "\n",
    "The following code is used to open a database to be able to access its tables and data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run Spark SQL queries, you must set up a Db2 Event Store Spark session. The EventSession class extends the optimizer of the SparkSession class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession = SparkSession.builder.appName(\"EventStore SQL in Python\").getOrCreate()\n",
    "eventSession = EventSession(sparkSession.sparkContext, DB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can execute the command to open the database in the event session you created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventSession.open_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access an existing table in the database\n",
    "The following code section retrieves the names of all tables that exist in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with EventContext.get_event_context(DB_NAME) as ctx:\n",
    "   print(\"Event context successfully retrieved.\")\n",
    "\n",
    "print(\"Table names:\")\n",
    "table_names = ctx.get_names_of_tables()\n",
    "for name in table_names:\n",
    "   print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the name of the existing table. We then load the table and get a DataFrame references to access the table with queries. The following code loads the tables and creates a temporary view with the same name as the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = eventSession.load_event_table(TABLE_NAME)\n",
    "tab.createOrReplaceTempView(TABLE_NAME)\n",
    "print(\"Table \" + TABLE_NAME + \" successfully loaded and temporary view created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code retrieves the schema of the table we want to investigate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    resolved_table_schema = ctx.get_table(TABLE_NAME)\n",
    "    print(resolved_table_schema)\n",
    "except Exception as err:\n",
    "    print(\"Table not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Modeling\n",
    "This section shows how to build a machine learning model with the data stored in the IBM Db2 Event Store database.\n",
    "\n",
    "### Recall from the *Event_Store_Data_Analytics* notebook\n",
    "- There are two input variables: ambient temperature and power consumption. The dependent variable is the sensor temperature reading.\n",
    "- All features follow normal distribution.\n",
    "- There is an obvious linear relationship between each independent variable and the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try generating a linear model to predict sensor temperature with power consumption and ambient temperature using the data stored in the IBM Db2 Event Store database table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the relevant PySpark machine learning libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell builds a new spark SQL DataFrame from the `tab` DataFrame, and prints out the `variable_df` DataFrame schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "variables = [\"ambient_temp\", \"power\"]\n",
    "variable_df = tab.select(col(\"temperature\").alias(\"label\"), *variables)\n",
    "variable_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the DataFrame into a training set and a test set at a percentage of 75 and 25.\n",
    "\n",
    "We first build and train the model on the training set, then evaluate the model performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = variable_df.randomSplit([0.75, 0.25], 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is built as a pipeline. There are three stages in the model pipeline: *vector assembly*, *standarization*, and *model definition*. \n",
    "\n",
    "In the following cell we execute the three stages.\n",
    "\n",
    "The training set is first assembled in to a dense vector. Then, the dense vector is standarized to a standard normal distribution. Finally, the linear model is defined with regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=variables, outputCol=\"unscaled_variables\")\n",
    "standardScaler = StandardScaler(inputCol=\"unscaled_variables\", outputCol=\"features\")\n",
    "linear_model = LinearRegression(maxIter=10, regParam=.01)\n",
    "\n",
    "stages = [vectorAssembler, standardScaler, linear_model]\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is then trained on the training set. The trained model is used to make predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(training)\n",
    "prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we show the first 10 rows out of the approximately 250 thousand in the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "The performance of the linear model we just built can be evaluated using multiple error metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load and define a regression evaluator using PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then evaluate the model performance with multiple error metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = evaluator.evaluate(prediction)\n",
    "\n",
    "mae = evaluator.evaluate(prediction, {evaluator.metricName: \"mae\"})\n",
    "\n",
    "r2 = evaluator.evaluate(prediction, {evaluator.metricName: \"r2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we put the error metrics into a dataframe to help visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_df = {\"r2\":r2, \"mae\":mae, \"rmse\":rmse}\n",
    "error_df = pd.DataFrame.from_dict(error_df, orient=\"index\")\n",
    "error_df.columns = [\"error metrics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show error metrics\n",
    "error_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Summarization**  \n",
    "The r2 metrics shows the percentage of the variance in the data that is explained by the model. Our model has a high r2 value that is very close to 1 -- meaning most of the variance in the test data can be explained with our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "Now that the model is trained, you can deploy the model. Once deployed, the model can be used to generate real-time online scoring on the data streamed into IBM Db2 Event Store.\n",
    "\n",
    "* If you are using the **Enterprise Edition** of Db2 Event Store, you can use the save function in the `dsx_ml` library.\n",
    "* If you are using the **Developer Edition** of Db2 Event Store, you need to add a Machine Learning service. You can use one with a trial account on IBM Cloud.\n",
    "  * Sign in and create the service [here](https://console.ng.bluemix.net/catalog/services/machine-learning).\n",
    "  * Click on `Service credentials` and then `New credential` and `Add`.\n",
    "  * Use `View credentials` and copy the credentials JSON.\n",
    "  * Use the JSON to set the `wml_credentials` variable below.\n",
    "  * After the pip install watson-machine-learning-client, you may need to restart your kernel and run the notebook again from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will attempt to initialize dsx_ml and set the use_cloud_ml toggle.\n",
    "# Later cells will use the use_cloud_ml toggle, to choose the necessary API.\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "use_cloud_ml = False\n",
    "try:\n",
    "    from dsx_ml.ml import save\n",
    "    if os.environ.get('DSX_TOKEN'):\n",
    "        print('Using dsx_ml to deploy model to IBM Db2 Event Store.')\n",
    "    else:\n",
    "        print('DSX_TOKEN not found, try using IBM Cloud Machine Learning.')\n",
    "        use_cloud_ml = True\n",
    "except ImportError:\n",
    "    print('Cannot import dsx_ml. Try using IBM Cloud Machine Learning.')\n",
    "    use_cloud_ml = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Db2 Event Store Developer Edition plus Machine Learning on IBM Cloud, save the model with metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# If you are using IBM Cloud for your ML deployment...\n",
    "#\n",
    "# * The use_cloud_ml toggle should be set to True.\n",
    "# * You need to set wml_credentials to your service credentials JSON.\n",
    "# * You most likely will need to restart your kernel after running the pip install (below).\n",
    "# * After the pip install runs once, you may want to comment out that line.\n",
    "\n",
    "def guid_from_space_name(client, space_name):\n",
    "     instance_details = client.service_instance.get_details()\n",
    "     space = client.spaces.get_details()\n",
    "     return(next(item for item in space['resources'] if item['entity'][\"name\"] == space_name)['metadata']['guid'])\n",
    "\n",
    "if use_cloud_ml:\n",
    "    print('Using IBM Cloud Machine Learning')\n",
    "    \n",
    "#     !pip install --user watson-machine-learning-client==1.0.364\n",
    "    !pip install --upgrade watson-machine-learning-client-V4==1.0.60\n",
    "    from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "    \n",
    "    # EDIT HERE TO SET YOUR CREDENTIALS:\n",
    "    wml_credentials = {\n",
    "        \"token\": bearerToken,\n",
    "        \"instance_id\" : \"wml_local\",\n",
    "        \"url\": \"https://\"+HOSTNAME,\n",
    "        \"version\": \"2.5.0\"\n",
    "    }\n",
    "    \n",
    "    \n",
    "    ## Create a deployment space from the Watson Studio  Access settings\n",
    "    client = WatsonMachineLearningAPIClient(wml_credentials)\n",
    "    space_uid = guid_from_space_name(client, DEPLOYMENT_SPACE)\n",
    "    client.set.default_space(space_uid)\n",
    "    \n",
    "#     client.runtimes.list(pre_defined=True,limit=100)\n",
    "#     client.repository.DefinitionMetaNames.get()\n",
    "    \n",
    "    meta_props={\n",
    "         client.repository.ModelMetaNames.NAME: \"Linear regression model to predict IOT sensor temperature\",\n",
    "         client.repository.ModelMetaNames.RUNTIME_UID: \"spark-mllib_2.3\",\n",
    "         client.repository.ModelMetaNames.TYPE: \"mllib_2.3\",\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Store the model\n",
    "    saved_model = client.repository.store_model(\n",
    "        model=model,\n",
    "        pipeline=pipeline,\n",
    "        training_data=training,\n",
    "        meta_props=meta_props)\n",
    "\n",
    "    published_model_uid = client.repository.get_model_uid(saved_model)\n",
    "    model_details = client.repository.get_details(published_model_uid)\n",
    "    print('Model Details:')\n",
    "    print(json.dumps(model_details, indent=2))\n",
    "    print('List Models:')\n",
    "    client.repository.list_models()\n",
    "\n",
    "    # Create an online deployment\n",
    "    \n",
    "    deploy_meta = {\n",
    "         client.deployments.ConfigurationMetaNames.NAME: \"Linear regression model to predict IOT sensor temperature\",\n",
    "         client.deployments.ConfigurationMetaNames.ONLINE: {}\n",
    "    }\n",
    "    \n",
    "    created_deployment = client.deployments.create(published_model_uid, meta_props=deploy_meta)\n",
    "    deployment_uid = client.deployments.get_uid(created_deployment)\n",
    "    print('Deployment uid = {}'.format(deployment_uid))\n",
    "    \n",
    "#     scoring_endpoint = client.deployments.get_scoring_url(created_deployment)\n",
    "#     print('Scoring Endpoint:')\n",
    "#     print(scoring_endpoint)\n",
    "    print('List Deployments')\n",
    "    client.deployments.list()\n",
    "    \n",
    "else:\n",
    "    print('Not using remote IBM Cloud Machine Learning.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Db2 Event Store Enterprise Edition, save the model with metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the saved model we then define a header that contains authorization, which will be sent to the endpoint, and then retrieve the endpoint to the saved model to allow us to externally access it. Note that the host name `dsxl-api` needs to be replaced with the corresponding external IP address of your IBM Watson Studio cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_cloud_ml:\n",
    "    model_name = \"Event_Store_IOT_Sensor_Temperature_Prediction_Model\"\n",
    "    saved_model = save(name=model_name, \n",
    "                       model=model,\n",
    "                       test_data=test,\n",
    "                       algorithm_type=\"Regression\",\n",
    "                       source='Event_Store_Modeling.ipynb',\n",
    "                       description=\"Linear regression model to predict IOT sensor temperature\"\n",
    "                      )\n",
    "\n",
    "    import os\n",
    "    import requests\n",
    "\n",
    "    header_online = {'Content-Type': 'application/json', 'Authorization': os.environ['DSX_TOKEN']}\n",
    "    # Retrieve the endpoint to the saved model\n",
    "    print(saved_model[\"scoring_endpoint\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Prediction with the Deployed Model\n",
    "Now the model has been saved and deployed. After deployment, the endpoint of model can be used to make a prediction for new data using the online scoring service.  \n",
    "\n",
    "The following sample code snippet calls the scoring endpoint to make predictions on the new data. The prediction can be made on single datum, or on batch data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a sample datum to be predicted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 new test data points\n",
    "new_data = {\"deviceID\" : 2, \"sensorID\": 24, \"ts\": 1541430459386, \"ambient_temp\": 30, \"power\": 10}\n",
    "new_data2 = {\"deviceID\" : 1, \"sensorID\": 12, \"ts\": 1541230400000, \"ambient_temp\": 16, \"power\": 50}\n",
    "\n",
    "job_payload = {\n",
    "   client.deployments.ScoringMetaNames.INPUT_DATA: [{\n",
    "     'fields':tuple(new_data.keys()),\n",
    "     'values': [tuple(new_data.values())]\n",
    "   }]\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Single datum prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cloud_ml:\n",
    "    predictions = client.deployments.score(\n",
    "        deployment_uid, job_payload)\n",
    "    print(json.dumps(predictions, indent=2))\n",
    "else:\n",
    "    payload_scoring = [new_data]\n",
    "    scoring_response = requests.post(saved_model[\"scoring_endpoint\"], json=payload_scoring, headers=header_online, verify=False)\n",
    "    print(scoring_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is a regression model, we can retrieve the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cloud_ml:\n",
    "    prediction_index = predictions[\"predictions\"][0][\"fields\"].index(\"prediction\")\n",
    "    # print(json.dumps(predictions, indent=2))\n",
    "    print(\"predictions: \", [value[prediction_index] for value in predictions[\"predictions\"][0][\"values\"]])\n",
    "else:\n",
    "    print(\"predictions: \", scoring_response.json()[\"object\"][\"output\"][\"predictions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Batch prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cloud_ml:\n",
    "    predictions = client.deployments.score(\n",
    "        deployment_uid, job_payload)\n",
    "    # print(json.dumps(predictions, indent=2))\n",
    "    print(\"predictions: \", [value[prediction_index] for value in predictions[\"predictions\"][0][\"values\"]])\n",
    "else:\n",
    "    payload_scoring = [new_data, new_data2]\n",
    "    scoring_response = requests.post(saved_model[\"scoring_endpoint\"], json=payload_scoring, headers=header_online, verify=False)\n",
    "    print(scoring_response.text)\n",
    "    print(\"predictions: \", scoring_response.json()[\"object\"][\"output\"][\"predictions\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This notebook introduced you to machine learning and model deployment with IBM Db2 Event Store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><font size=-1 color=gray>\n",
    "&copy; Copyright 2019 IBM Corp. All Rights Reserved.\n",
    "<p>\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file\n",
    "except in compliance with the License. You may obtain a copy of the License at\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the\n",
    "License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
    "express or implied. See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "</font></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
