{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying IBM Db2 Event Store\n",
    "IBM Db2 Event Store is a hybrid transactional/analytical processing (HTAP) system. This notebook will demonstrate the best practices for querying a table stored in IBM Db2 Event Store.\n",
    "\n",
    "***Pre-Req: Event_Store_Table_Creation***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to IBM Db2 Event Store\n",
    "\n",
    "Edit the values in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONNECTION_ENDPOINT=\"\"\n",
    "\n",
    "EVENT_USER_ID=\"\"\n",
    "\n",
    "EVENT_PASSWORD=\"\"\n",
    "\n",
    "# Port will be 1100 for version 1.1.2 or later (5555 for version 1.1.1)\n",
    "PORT = \"30370\"\n",
    "\n",
    "DEPLOYMENT_ID=\"\"\n",
    "\n",
    "# Database name\n",
    "DB_NAME = \"EVENTDB\"\n",
    "\n",
    "# Table name\n",
    "TABLE_NAME = \"IOT_TEMPERATURE\"\n",
    "\n",
    "HOSTNAME=\"\"\n",
    "\n",
    "DEPLOYMENT_SPACE=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: Only run this cell if your IBM Db2 Event Store is installed with IBM Cloud Pak for Data (CP4D)\n",
    "\n",
    "# In IBM Cloud Pak for Data, we need to create link to ensure Event Store Python library is \n",
    "# properly exposed to the Spark runtime.\n",
    "import os\n",
    "src = '/home/spark/user_home/eventstore/eventstore'\n",
    "dst = '/home/spark/shared/user-libs/python3.6/eventstore'\n",
    "try:\n",
    "    os.remove(dst)\n",
    "except EnvironmentError as e:\n",
    "    print(\"Symlink doesn't exist, creating symlink to include Event Store Python library...\")\n",
    "os.symlink(src, dst)\n",
    "print(\"Creating symlink to include Event Store Python library...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bearerToken=!echo `curl --silent -k -X GET https://{HOSTNAME}:443/v1/preauth/validateAuth -u admin:password |python -c \"import sys, json; print(json.load(sys.stdin)['accessToken'])\"`\n",
    "bearerToken=bearerToken[0]\n",
    "keystorePassword=!echo `curl -k --silent  GET -H \"authorization: Bearer {bearerToken}\" \"https://{HOSTNAME}:443/icp4data-databases/{DEPLOYMENT_ID}/zen/com/ibm/event/api/v1/oltp/keystore_password\"`\n",
    "keystorePassword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eventstore.common import ConfigurationReader\n",
    "from eventstore.oltp import EventContext\n",
    "from eventstore.sql import EventSession\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Event Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfigurationReader.setConnectionEndpoints(CONNECTION_ENDPOINT)\n",
    "ConfigurationReader.setEventUser(EVENT_USER_ID)\n",
    "ConfigurationReader.setEventPassword(EVENT_PASSWORD)\n",
    "ConfigurationReader.setSslKeyAndTrustStorePasswords(keystorePassword[0])\n",
    "ConfigurationReader.setDeploymentID(DEPLOYMENT_ID)\n",
    "ConfigurationReader.getSslTrustStorePassword()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the database\n",
    "\n",
    "The cells in this section are used to open the database and create a temporary view for the table that we created previously.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run Spark SQL queries, you first have to set up a Db2 Event Store Spark session. The EventSession class extends the optimizer of the SparkSession class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession = SparkSession.builder.appName(\"EventStore SQL in Python\").getOrCreate()\n",
    "eventSession = EventSession(sparkSession.sparkContext, DB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell opens the database to allow operations against it to be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eventSession.open_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following cells, we can list all existing tables and then load the table we previously created into the tab DataFrame reference. Note that we are defining the `tab` DataFrame reference that will be used later on in this notebook to create a temporary view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with EventContext.get_event_context(DB_NAME) as ctx:\n",
    "   print(\"Event context successfully retrieved.\")\n",
    "\n",
    "print(\"Table names:\")\n",
    "table_names = ctx.get_names_of_tables()\n",
    "for name in table_names:\n",
    "   print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = eventSession.load_event_table(TABLE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recall the table schema we previously created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    resolved_table_schema = ctx.get_table(TABLE_NAME)\n",
    "    print(resolved_table_schema)\n",
    "except Exception as err:\n",
    "    print(\"Table not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for efficient queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we create a lazily evaluated \"view\" that we can then use like a hive table in Spark SQL, but this is only evaluated when we actually run or cache query results. We are calling this view \"readings\" and that is how we will refer to it in the queries below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab.createOrReplaceTempView(\"readings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"SELECT count(*) FROM readings\"\n",
    "print(\"{}\\nRunning query in Event Store...\".format(query))\n",
    "df_data = eventSession.sql(query)\n",
    "df_data.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the record structure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM readings LIMIT 1\"\n",
    "print(\"{}\\nRunning query in Event Store...\".format(query))\n",
    "df_data = eventSession.sql(query)\n",
    "df_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT MIN(ts), MAX(ts) FROM readings\"\n",
    "print(\"{}\\nRunning query in Event Store...\".format(query))\n",
    "df_data = eventSession.sql(query)\n",
    "df_data.toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal query through the index\n",
    "\n",
    "- Index queries will significantly reduce amount of data that needs to be scanned for results. \n",
    "    - Indexes in IBM Db2 Event Store are formed asynchronously to avoid insert latency. \n",
    "    - They are stored as a Log Structured Merge (LSM) Tree.\n",
    "    - The index is formed by \"runs\", which include sequences of sorted keys. These runs are written to disk during “Share” processing.\n",
    "    - These index runs are merged together over time to improve scan and I/O efficiency.\n",
    "\n",
    "- For an optimal query performance you must specify equality on all the equal_columns in the index and a range on the sort column in the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, in the following query we are retrieving all the values in the range of dates for a specific device and sensor, where both the `deviceID` and `sensorID` are in the equal_columns definition for the index schema and the `ts` column is the sort column for the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the following cell runs the query and caches the results. Note that this caching is for demostration purposes, to show the time it takes to run the query and cache the results in memory within Spark. This caching is recommended when you are going to do additional processing on this cached data, as the query against IBM Db2 Event Store is only run once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "index_query = \"SELECT ts, temperature  FROM readings where deviceID=1 and sensorID=12 and ts >1541021271619 and ts < 1541043671128 order by ts\"\n",
    "print(\"{}\\nCreating a dataframe for the query ...\".format(index_query))\n",
    "df_index_query = eventSession.sql(index_query)\n",
    "df_index_query.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the results from the cached data can be visualized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_index_query.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-optimal query\n",
    "\n",
    "This next query shows a sub-optimal query that only specifies equality in one of the equal_columns in the index schema, and for this reason ends up doing a full scan of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fullscan_query = \"SELECT count(*) FROM readings where sensorID = 7\"\n",
    "print(\"{}\\nCreating a dataframe for the query...\".format(fullscan_query))\n",
    "df_fullscan_query = eventSession.sql(fullscan_query)\n",
    "df_fullscan_query.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fullscan_query.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing multiple sensorIDs optimally\n",
    "\n",
    "The easiest way to write a query that needs to retrieve multiple values in the equal_columns in the index schema is by using an *In-List*. With this, you can get optimal index access across multiple sensorID's. \n",
    "\n",
    "In this example we specify equality for a specific deviceID, and an In-List for the four sensors we are trying to retrieve. To limit the number of records we are returning we also include a range of timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inlist_query = \"SELECT deviceID, sensorID, ts FROM readings where deviceID=1 and sensorID in (1, 5, 7, 12) and ts >1541021271619 and ts < 1541043671128 order by ts\"\n",
    "print(\"{}\\nCreating a dataframe for the query...\".format(inlist_query))\n",
    "df_inlist_query = eventSession.sql(inlist_query)\n",
    "df_inlist_query.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inlist_query.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploiting the synopsis table:  Advanced medium weight queries\n",
    "\n",
    "- Event Store tables include a synopsis table which summarizes the minimum/maximum values of the data for each range of rows in the synopsis table. \n",
    "\n",
    "     - It contains one range for every 1000 rows.\n",
    "     - It is stored in a separate internal table in the shared storage layer.\n",
    "     - It is parquet compressed to minimize footprint.\n",
    "     - For highly selective queries, it can improve performance by up to 1000x.\n",
    "\n",
    "- Using an equality or a range predicate on a clustered field (e.g. the 'ts' column in our case because the data is inserted into the table in order) is faster than doing a full scan as it should be able to exploit the synopsis table, but this will be slower than an optimal index scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "synopsis_query = \"SELECT deviceID, sensorID, ts FROM readings where ts >1541021271619 and ts < 1541043671128 order by ts\"\n",
    "print(\"{}\\nCreating a dataframe for the query...\".format(synopsis_query))\n",
    "df_synopsis_query = eventSession.sql(synopsis_query)\n",
    "df_synopsis_query.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_synopsis_query.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This demo introduced you to the best practices querying the table stored in IBM Db2 Event Store database.\n",
    "\n",
    "## Next Step\n",
    "`\"Event_Store_Data_Analytics.ipynb\"` will show you how to perform data analytics with IBM Db2 Event Store with multiple scientific tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p><font size=-1 color=gray>\n",
    "&copy; Copyright 2019 IBM Corp. All Rights Reserved.\n",
    "<p>\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file\n",
    "except in compliance with the License. You may obtain a copy of the License at\n",
    "https://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the\n",
    "License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either\n",
    "express or implied. See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "</font></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
